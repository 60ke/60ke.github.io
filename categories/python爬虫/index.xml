<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>python爬虫 on LookForAdmin的博客</title><link>https://60ke.github.io/categories/python%E7%88%AC%E8%99%AB/</link><description>Recent content in python爬虫 on LookForAdmin的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Wed, 01 Aug 2018 10:54:00 +0000</lastBuildDate><atom:link href="https://60ke.github.io/categories/python%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml"/><item><title>requests中遇到的一些问题</title><link>https://60ke.github.io/drafts/requests%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</link><pubDate>Wed, 01 Aug 2018 10:54:00 +0000</pubDate><guid>https://60ke.github.io/drafts/requests%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</guid><description>今天使用Python的requests模拟posts时总是,无法获取到正确的结果,后来使用postman的时候,在发送请求的时候在body中奖数据类型由默认的Text改为json可以成功,但是使用postman生成的python代码,并加入了正确的cookie还是不能获得正常的返</description></item><item><title>微博搜索爬虫</title><link>https://60ke.github.io/drafts/%E5%BE%AE%E5%8D%9A%E6%90%9C%E7%B4%A2%E7%88%AC%E8%99%AB/</link><pubDate>Thu, 05 Jul 2018 08:18:46 +0000</pubDate><guid>https://60ke.github.io/drafts/%E5%BE%AE%E5%8D%9A%E6%90%9C%E7%B4%A2%E7%88%AC%E8%99%AB/</guid><description>爬虫包括了登录,验证码验证模块 代码放到github上了微博搜索爬虫</description></item><item><title>scrapy之FormRequest模拟post请求</title><link>https://60ke.github.io/drafts/scrapy%E4%B9%8BFormRequest%E6%A8%A1%E6%8B%9Fpost%E8%AF%B7%E6%B1%82/</link><pubDate>Tue, 02 Jan 2018 07:36:19 +0000</pubDate><guid>https://60ke.github.io/drafts/scrapy%E4%B9%8BFormRequest%E6%A8%A1%E6%8B%9Fpost%E8%AF%B7%E6%B1%82/</guid><description>先放代码吧,以后闲了再完善 def start_requests(self): url = &amp;quot;http://www.hebzx.gov.cn/specialnews.aspx?meetingtype=009001&amp;quot; yield scrapy.Request(url,callback=self.parse0) def parse0(self,response): formdata = {&amp;quot;__EVENTTARGET&amp;quot;:&amp;quot;AspNetPager1&amp;quot;,&amp;quot;__EVENTARGUMENT&amp;quot;:&amp;quot;2&amp;quot;} return scrapy.FormRequest.from_response( response, formdata=formdata, callback=self.parse1 ) def parse1(self,response): print(&amp;quot;1111111111111&amp;quot;) print(response.text)</description></item><item><title>scrapy保存数据为多个json</title><link>https://60ke.github.io/drafts/scrapy%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E4%B8%BA%E5%A4%9A%E4%B8%AAjson/</link><pubDate>Wed, 23 Aug 2017 06:26:33 +0000</pubDate><guid>https://60ke.github.io/drafts/scrapy%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E4%B8%BA%E5%A4%9A%E4%B8%AAjson/</guid><description>定义pipline： # -*- coding: utf-8 -*- # Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html # class VmwarePipeline(object): # def process_item(self, item, spider): # return item import json import codecs class VmwarePipeline(object): def process_item(self, item, spider): self.file = codecs.open('%s.json'%item['vid'], 'w', encoding='utf-8') line = json.dumps(dict(item), ensure_ascii=False) + &amp;quot;\n&amp;quot; self.file.write(line) return item def spider_closed(self, spider): self.file.close() PS：scrapy中spiders的解析不能执行的问题 目前为止遇到的情况有两种 解析函数本身存在问题,解析错误导致不能继续传递 解析的</description></item><item><title>scrapy之获取ubuntu安全公告,并将cve存储为json</title><link>https://60ke.github.io/drafts/scrapy%E4%B9%8B%E8%8E%B7%E5%8F%96ubuntu%E5%AE%89%E5%85%A8%E5%85%AC%E5%91%8A%E5%B9%B6%E5%B0%86cve%E5%AD%98%E5%82%A8%E4%B8%BAjson/</link><pubDate>Tue, 22 Aug 2017 03:33:00 +0000</pubDate><guid>https://60ke.github.io/drafts/scrapy%E4%B9%8B%E8%8E%B7%E5%8F%96ubuntu%E5%AE%89%E5%85%A8%E5%85%AC%E5%91%8A%E5%B9%B6%E5%B0%86cve%E5%AD%98%E5%82%A8%E4%B8%BAjson/</guid><description>创建ubuntu项目 scrapy startproject ubuntu 创建spider模板 cd ubuntu scrapy genspider Ubuntu https://usn.ubuntu.com cat ubuntu/ubuntu/spiders/Ubuntu.py # -*- coding: utf-8 -*- import scrapy class UbuntuSpider(scrapy.Spider): name = 'Ubuntu' allowed_domains = ['https://usn.ubuntu.com'] start_urls = ['http://https://usn.ubuntu.com/'] def parse(self, response): pass ~ 编写spider # -*- coding: utf-8 -*- import scrapy from bs4 import BeautifulSoup import re import requests from ubuntu.items import UbuntuItem class UbuntuSpider(scrapy.Spider): name = 'Ubuntu' allowed_domains = ['usn.ubuntu.com'] def start_requests(self): url = &amp;quot;https://usn.ubuntu.com/usn/&amp;quot; content = requests.get(url).text soup = BeautifulSoup(content,&amp;quot;html.parser&amp;quot;) page = soup.find(attrs={&amp;quot;class&amp;quot;:&amp;quot;right&amp;quot;}).text.strip() max_page = re.findall(&amp;quot;Showing page 1 of (.+?) &amp;quot;,page)[0] start_urls = [] for page in range(int(max_page)): page +=1 url = 'https://usn.ubuntu.com/usn/?page=%s'%page yield scrapy.Request(url,callback=self.parse0) def parse0(self, response): # 获取公告中usn的cv</description></item><item><title>scrapy-xpath用法</title><link>https://60ke.github.io/drafts/scrapy-xpath%E7%94%A8%E6%B3%95/</link><pubDate>Tue, 22 Aug 2017 02:04:10 +0000</pubDate><guid>https://60ke.github.io/drafts/scrapy-xpath%E7%94%A8%E6%B3%95/</guid><description>&lt;p>转载自http://www.cnblogs.com/huhuuu/p/3701017.html　　
Scrapy是基于python的开源爬虫框架,使用起来也比较方便。具体的官网档：http://doc.scrapy.org/en/latest/&lt;/p>
&lt;p>　　之前以为了解python就可以直接爬网站了,原来还要了解HTML,XML的基本协议,在了解基础以后,在了解下xpath的基础上,再使用正则表达式(python下的re包提供支持)提取一定格式的信息（比如说url）,就比较容易处理网页了。&lt;/p>
&lt;p>　　xpath是Scrapy下快速提取特定信息（如title,head,href等）的一个接口。&lt;/p></description></item><item><title>python爬虫之——网页解析</title><link>https://60ke.github.io/drafts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/</link><pubDate>Thu, 17 Aug 2017 10:18:00 +0000</pubDate><guid>https://60ke.github.io/drafts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/</guid><description>常用的正则 首先导入第三方包 import re 常用的正则函数： re.findall() re.findall()这个函数之前用的已经比较多了,除了以前提到的re.findall(&amp;quot;&amp;quot;,target)在“”中放入（.+?）来匹配要查找到内容,还可以结合python中 r+“”的特性,来进行正则查找,同时</description></item><item><title>爬虫入门到进阶-1--保存一张图片</title><link>https://60ke.github.io/drafts/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6-1--%E4%BF%9D%E5%AD%98%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%89%87/</link><pubDate>Mon, 26 Jun 2017 16:18:39 +0000</pubDate><guid>https://60ke.github.io/drafts/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%88%B0%E8%BF%9B%E9%98%B6-1--%E4%BF%9D%E5%AD%98%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%89%87/</guid><description/></item></channel></rss>