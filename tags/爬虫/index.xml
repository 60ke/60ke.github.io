<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>爬虫 on LookForAdmin</title><link>https://60ke.github.io/tags/%E7%88%AC%E8%99%AB/</link><description>Recent content in 爬虫 on LookForAdmin</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 22 Apr 2017 09:39:41 +0000</lastBuildDate><atom:link href="https://60ke.github.io/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml"/><item><title>转载-Python 爬虫：把廖雪峰的教程转换成 PDF 电子书</title><link>https://60ke.github.io/drafts/%E8%BD%AC%E8%BD%BD-python-%E7%88%AC%E8%99%AB%E6%8A%8A%E5%BB%96%E9%9B%AA%E5%B3%B0%E7%9A%84%E6%95%99%E7%A8%8B%E8%BD%AC%E6%8D%A2%E6%88%90-pdf-%E7%94%B5%E5%AD%90%E4%B9%A6/</link><pubDate>Sat, 22 Apr 2017 09:39:41 +0000</pubDate><guid>https://60ke.github.io/drafts/%E8%BD%AC%E8%BD%BD-python-%E7%88%AC%E8%99%AB%E6%8A%8A%E5%BB%96%E9%9B%AA%E5%B3%B0%E7%9A%84%E6%95%99%E7%A8%8B%E8%BD%AC%E6%8D%A2%E6%88%90-pdf-%E7%94%B5%E5%AD%90%E4%B9%A6/</guid><description>&lt;p>来源：https://github.com/lzjun567/crawler_html2pdf/blob/master/pdf/crawler.py&lt;/p>
&lt;h1 id="codingutf-8">coding=utf-8&lt;/h1>
&lt;pre>&lt;code>from __future__ import unicode_literals
import logging
import os
import re
import time
try:
from urllib.parse import urlparse # py3
except:
from urlparse import urlparse # py2
import pdfkit
import requests
from bs4 import BeautifulSoup
html_template = &amp;quot;&amp;quot;&amp;quot;
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
{content}
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&amp;quot;&amp;quot;&amp;quot;
class Crawler(object):
&amp;quot;&amp;quot;&amp;quot;
爬虫基类,所有爬虫都应该继承此类
&amp;quot;&amp;quot;&amp;quot;
name = None
def __init__(self, name, start_url):
&amp;quot;&amp;quot;&amp;quot;
初始化
:param name: 保存问的PDF文件名,不需要后缀名
:param start_url: 爬虫入口URL
&amp;quot;&amp;quot;&amp;quot;
self.name = name
self.start_url = start_url
self.domain = '{uri.scheme}://{uri.netloc}'.format(uri=urlparse(self.start_url))
def crawl(self, url):
&amp;quot;&amp;quot;&amp;quot;
pass
:return:
&amp;quot;&amp;quot;&amp;quot;
print(url)
response = requests.get(url)
return response
def parse_menu(self, response):
&amp;quot;&amp;quot;&amp;quot;
解析目录结构,获取所有URL目录列表:由子类实现
:param response 爬虫返回的response对象
:return: url 可迭代对象(iterable) 列表,生成器,元组都可以
&amp;quot;&amp;quot;&amp;quot;
raise NotImplementedError
def parse_body(self, response):
&amp;quot;&amp;quot;&amp;quot;
解析正文,由子类实现
:param response: 爬虫返回的response对象
:return: 返回经过处理的html文本
&amp;quot;&amp;quot;&amp;quot;
raise NotImplementedError
def run(self):
start = time.time()
options = {
'page-size': 'Letter',
'margin-top': '0.75in',
'margin-right': '0.75in',
'margin-bottom': '0.75in',
'margin-left': '0.75in',
'encoding': &amp;quot;UTF-8&amp;quot;,
'custom-header': [
('Accept-Encoding', 'gzip')
],
'cookie': [
('cookie-name1', 'cookie-value1'),
('cookie-name2', 'cookie-value2'),
],
'outline-depth': 10,
}
htmls = []
for index, url in enumerate(self.parse_menu(self.crawl(self.start_url))):
html = self.parse_body(self.crawl(url))
f_name = &amp;quot;.&amp;quot;.join([str(index), &amp;quot;html&amp;quot;])
with open(f_name, 'wb') as f:
f.write(html)
htmls.append(f_name)
pdfkit.from_file(htmls, self.name + &amp;quot;.pdf&amp;quot;, options=options)
for html in htmls:
os.remove(html)
total_time = time.time() - start
print(u&amp;quot;总共耗时：%f 秒&amp;quot; % total_time)
class LiaoxuefengPythonCrawler(Crawler):
&amp;quot;&amp;quot;&amp;quot;
廖雪峰Python3教程
&amp;quot;&amp;quot;&amp;quot;
def parse_menu(self, response):
&amp;quot;&amp;quot;&amp;quot;
解析目录结构,获取所有URL目录列表
:param response 爬虫返回的response对象
:return: url生成器
&amp;quot;&amp;quot;&amp;quot;
soup = BeautifulSoup(response.content, &amp;quot;html.parser&amp;quot;)
menu_tag = soup.find_all(class_=&amp;quot;uk-nav uk-nav-side&amp;quot;)[1]
for li in menu_tag.find_all(&amp;quot;li&amp;quot;):
url = li.a.get(&amp;quot;href&amp;quot;)
if not url.startswith(&amp;quot;http&amp;quot;):
url = &amp;quot;&amp;quot;.join([self.domain, url]) # 补全为全路径
yield url
def parse_body(self, response):
&amp;quot;&amp;quot;&amp;quot;
解析正文
:param response: 爬虫返回的response对象
:return: 返回处理后的html文本
&amp;quot;&amp;quot;&amp;quot;
try:
soup = BeautifulSoup(response.content, 'html.parser')
body = soup.find_all(class_=&amp;quot;x-wiki-content&amp;quot;)[0]
# 加入标题, 居中显示
title = soup.find('h4').get_text()
center_tag = soup.new_tag(&amp;quot;center&amp;quot;)
title_tag = soup.new_tag('h1')
title_tag.string = title
center_tag.insert(1, title_tag)
body.insert(1, center_tag)
html = str(body)
# body中的img标签的src相对路径的改成绝对路径
pattern = &amp;quot;(&amp;lt;img .*?src=\&amp;quot;)(.*?)(\&amp;quot;)&amp;quot;
def func(m):
if not m.group(3).startswith(&amp;quot;http&amp;quot;):
rtn = &amp;quot;&amp;quot;.join([m.group(1), self.domain, m.group(2), m.group(3)])
return rtn
else:
return &amp;quot;&amp;quot;.join([m.group(1), m.group(2), m.group(3)])
html = re.compile(pattern).sub(func, html)
html = html_template.format(content=html)
html = html.encode(&amp;quot;utf-8&amp;quot;)
return html
except Exception as e:
logging.error(&amp;quot;解析错误&amp;quot;, exc_info=True)
if __name__ == '__main__':
start_url = &amp;quot;http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000&amp;quot;
crawler = LiaoxuefengPythonCrawler(&amp;quot;廖雪峰Git&amp;quot;, start_url)
crawler.run()&lt;/code>&lt;/pre></description></item><item><title>煎蛋模块化爬虫</title><link>https://60ke.github.io/drafts/%E7%85%8E%E8%9B%8B%E6%A8%A1%E5%9D%97%E5%8C%96%E7%88%AC%E8%99%AB/</link><pubDate>Fri, 21 Apr 2017 12:46:00 +0000</pubDate><guid>https://60ke.github.io/drafts/%E7%85%8E%E8%9B%8B%E6%A8%A1%E5%9D%97%E5%8C%96%E7%88%AC%E8%99%AB/</guid><description>&lt;p>按照小甲鱼的Python课程打出来的,在find_imgs中添加了http:（未使用正则表达式）&lt;/p>
&lt;h2 id="煎蛋模块化爬虫">煎蛋模块化爬虫&lt;/h2>
&lt;pre>&lt;code>import os
import urllib.request
def url_open(url):
req = urllib.request.Request(url)
req.add_header('User-Agent', '')
response = urllib.request.urlopen(url)
html = response.read()
return html
def get_page(url):
html = url_open(url).decode('utf-8')
a = html.find('current-comment-page') + 23
b = html.find(']',a)
return html[a:b]
def find_imgs(url):
html = url_open(url).decode('utf-8')
img_addrs = []
a = html.find('img src=')
while a != -1:
b = html.find('.jpg', a, a+255)
if b != -1:
img_addrs.append('http:'+html[a+9:b+4])
else:
b =a + 9
a = html.find('img src=', b)
return img_addrs
def save_imgs(folder, img_addrs):
for each in img_addrs:
filename = each.split('/')[-1]
with open(filename, 'wb') as f:
img = url_open(each)
f.write(img)
def download_mm(folder= 'OOXX', pages=100):
os.mkdir(folder)
os.chdir(folder)
url = 'http://jandan.net/ooxx/'
page_num = int(get_page(url))
for i in range(pages):
page_num -=i
page_url = url + 'page-' +str(page_num)
img_addrs = find_imgs(page_url)
save_imgs(folder, img_addrs)
download_mm()#//因为有默认参数所以（）可以为空,没有默认参数时不可以
&lt;/code>&lt;/pre>
&lt;h2 id="find用法">find()用法：&lt;/h2>
&lt;p>#!/usr/bin/python&lt;/p>
&lt;pre>&lt;code>str1 = &amp;quot;this is string example....wow!!!&amp;quot;
str2 = &amp;quot;exam&amp;quot;
print (str1.find(str2))
print (str1.find(str2, 10))
print (str1.find(str2, 10,12))
print (str1.find(str2, 40))
&lt;/code>&lt;/pre>
&lt;p>输出：
15
15
-1
-1&lt;/p></description></item><item><title>煎蛋网妹子图抓取</title><link>https://60ke.github.io/drafts/%E7%85%8E%E8%9B%8B%E7%BD%91%E5%A6%B9%E5%AD%90%E5%9B%BE%E6%8A%93%E5%8F%96/</link><pubDate>Fri, 21 Apr 2017 08:33:05 +0000</pubDate><guid>https://60ke.github.io/drafts/%E7%85%8E%E8%9B%8B%E7%BD%91%E5%A6%B9%E5%AD%90%E5%9B%BE%E6%8A%93%E5%8F%96/</guid><description>&lt;p>代码写的很烂,不过目的总算达到了&lt;/p>
&lt;p>import requests
from bs4 import BeautifulSoup
import re
import os&lt;/p>
&lt;pre>&lt;code>i = 1
url = 'http://jandan.net/ooxx/page-'+str(id)+'#comments'
for id in range(200):
res = requests.get('http://jandan.net/ooxx/page-'+str(id)+'#comments')
#print(res)
target_text = BeautifulSoup(res.text, 'html.parser')
target_img = target_text.select('img')
for img in target_img:
img = str(img)
img = re.findall(r'&amp;lt;img src=&amp;quot;//(.+?)&amp;quot;/&amp;gt;',img)
for picurl in img:
picurl = 'http://'+picurl
print('正在下载第'+i'张图片')
pic= requests.get(picurl)
string = 'tupia\\'+str(i) + '.jpg'##给要保存的文件命名（第一个'\'是用来转义后面的'\',）
fp = open(string,'wb')
fp.write(pic.content)
fp.close()
i += 1
&lt;/code>&lt;/pre>
&lt;p>“战果”：
&lt;img src="https://ws1.sinaimg.cn/mw690/6cf740f6ly1feudw60cz1j20nb0fsam1.jpg"
loading="lazy"
>&lt;/p></description></item><item><title>爬取起点中文网月票榜前500名网络小说</title><link>https://60ke.github.io/drafts/%E7%88%AC%E5%8F%96%E8%B5%B7%E7%82%B9%E4%B8%AD%E6%96%87%E7%BD%91%E6%9C%88%E7%A5%A8%E6%A6%9C%E5%89%8D500%E5%90%8D%E7%BD%91%E7%BB%9C%E5%B0%8F%E8%AF%B4/</link><pubDate>Wed, 19 Apr 2017 11:10:25 +0000</pubDate><guid>https://60ke.github.io/drafts/%E7%88%AC%E5%8F%96%E8%B5%B7%E7%82%B9%E4%B8%AD%E6%96%87%E7%BD%91%E6%9C%88%E7%A5%A8%E6%A6%9C%E5%89%8D500%E5%90%8D%E7%BD%91%E7%BB%9C%E5%B0%8F%E8%AF%B4/</guid><description>&lt;p>&amp;rsquo;''&lt;/p>
&lt;h2 id="参考自知乎httpszhuanlanzhihucomp26255754">参考自知乎https://zhuanlan.zhihu.com/p/26255754&lt;/h2>
&lt;h2 id="talk-is-cheap--show-your-code">Talk is cheap , show your code&lt;/h2>
&lt;p>所需模块： requests padndas BeautifulSoup&lt;/p>
&lt;p>import requests
from bs4 import BeautifulSoup
import pandas
newsary=[]
for i in range(1,26):
res=requests.get(&amp;lsquo;&lt;a class="link" href="http://r.qidian.com/yuepiao?chn=-1&amp;amp;page=%27&amp;#43;str%28i%29" target="_blank" rel="noopener"
>http://r.qidian.com/yuepiao?chn=-1&amp;page='+str(i)&lt;/a>)&lt;/p>
&lt;pre>&lt;code> soup=BeautifulSoup(res.text,'html.parser')
for news in soup.select('.rank-view-list li'):
newsary.append({'title':news.select('a')[1].text,'name':news.select('a')[2].text,'style':news.select('a')[3].text,'describe':news.select('p')[1].text,'lastest':news.select('p')[2].text,'url':news.select('a')[0]['href']})
newsdf=pandas.DataFrame(newsary)
newsdf.to_excel('qidiantop500.xlsx')
&lt;/code>&lt;/pre>
&lt;p>成果截图：&lt;/p>
&lt;p>&lt;img src="https://ws1.sinaimg.cn/large/6cf740f6ly1fes77bwrknj20qy0g2dhg.jpg"
loading="lazy"
>&lt;/p></description></item></channel></rss>