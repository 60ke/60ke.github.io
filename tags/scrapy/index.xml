<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>scrapy on LookForAdmin的博客</title><link>https://60ke.github.io/tags/scrapy/</link><description>Recent content in scrapy on LookForAdmin的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Wed, 23 Aug 2017 06:26:33 +0000</lastBuildDate><atom:link href="https://60ke.github.io/tags/scrapy/index.xml" rel="self" type="application/rss+xml"/><item><title>scrapy保存数据为多个json</title><link>https://60ke.github.io/drafts/scrapy%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E4%B8%BA%E5%A4%9A%E4%B8%AAjson/</link><pubDate>Wed, 23 Aug 2017 06:26:33 +0000</pubDate><guid>https://60ke.github.io/drafts/scrapy%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E4%B8%BA%E5%A4%9A%E4%B8%AAjson/</guid><description>定义pipline： # -*- coding: utf-8 -*- # Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html # class VmwarePipeline(object): # def process_item(self, item, spider): # return item import json import codecs class VmwarePipeline(object): def process_item(self, item, spider): self.file = codecs.open('%s.json'%item['vid'], 'w', encoding='utf-8') line = json.dumps(dict(item), ensure_ascii=False) + &amp;quot;\n&amp;quot; self.file.write(line) return item def spider_closed(self, spider): self.file.close() PS：scrapy中spiders的解析不能执行的问题 目前为止遇到的情况有两种 解析函数本身存在问题,解析错误导致不能继续传递 解析的</description></item><item><title>scrapy之获取ubuntu安全公告,并将cve存储为json</title><link>https://60ke.github.io/drafts/scrapy%E4%B9%8B%E8%8E%B7%E5%8F%96ubuntu%E5%AE%89%E5%85%A8%E5%85%AC%E5%91%8A%E5%B9%B6%E5%B0%86cve%E5%AD%98%E5%82%A8%E4%B8%BAjson/</link><pubDate>Tue, 22 Aug 2017 03:33:00 +0000</pubDate><guid>https://60ke.github.io/drafts/scrapy%E4%B9%8B%E8%8E%B7%E5%8F%96ubuntu%E5%AE%89%E5%85%A8%E5%85%AC%E5%91%8A%E5%B9%B6%E5%B0%86cve%E5%AD%98%E5%82%A8%E4%B8%BAjson/</guid><description>创建ubuntu项目 scrapy startproject ubuntu 创建spider模板 cd ubuntu scrapy genspider Ubuntu https://usn.ubuntu.com cat ubuntu/ubuntu/spiders/Ubuntu.py # -*- coding: utf-8 -*- import scrapy class UbuntuSpider(scrapy.Spider): name = 'Ubuntu' allowed_domains = ['https://usn.ubuntu.com'] start_urls = ['http://https://usn.ubuntu.com/'] def parse(self, response): pass ~ 编写spider # -*- coding: utf-8 -*- import scrapy from bs4 import BeautifulSoup import re import requests from ubuntu.items import UbuntuItem class UbuntuSpider(scrapy.Spider): name = 'Ubuntu' allowed_domains = ['usn.ubuntu.com'] def start_requests(self): url = &amp;quot;https://usn.ubuntu.com/usn/&amp;quot; content = requests.get(url).text soup = BeautifulSoup(content,&amp;quot;html.parser&amp;quot;) page = soup.find(attrs={&amp;quot;class&amp;quot;:&amp;quot;right&amp;quot;}).text.strip() max_page = re.findall(&amp;quot;Showing page 1 of (.+?) &amp;quot;,page)[0] start_urls = [] for page in range(int(max_page)): page +=1 url = 'https://usn.ubuntu.com/usn/?page=%s'%page yield scrapy.Request(url,callback=self.parse0) def parse0(self, response): # 获取公告中usn的cv</description></item><item><title>scrapy-xpath用法</title><link>https://60ke.github.io/drafts/scrapy-xpath%E7%94%A8%E6%B3%95/</link><pubDate>Tue, 22 Aug 2017 02:04:10 +0000</pubDate><guid>https://60ke.github.io/drafts/scrapy-xpath%E7%94%A8%E6%B3%95/</guid><description>&lt;p>转载自http://www.cnblogs.com/huhuuu/p/3701017.html　　
Scrapy是基于python的开源爬虫框架,使用起来也比较方便。具体的官网档：http://doc.scrapy.org/en/latest/&lt;/p>
&lt;p>　　之前以为了解python就可以直接爬网站了,原来还要了解HTML,XML的基本协议,在了解基础以后,在了解下xpath的基础上,再使用正则表达式(python下的re包提供支持)提取一定格式的信息（比如说url）,就比较容易处理网页了。&lt;/p>
&lt;p>　　xpath是Scrapy下快速提取特定信息（如title,head,href等）的一个接口。&lt;/p></description></item></channel></rss>